# A Glued Quantisation Lattice for Images

A [2023 paper] by Agrell, Pook-Kolb and Allen observes that glued lattices make better quantisation lattices than product lattices. Like everybody else, in my image compression algorithms I have until now been quantising images onto a product lattice. This document describes how I might instead use a glued lattice.

[recent paper]: https://arxiv.org/pdf/2312.00481.pdf

## Background

Compressing an image involves three main concepts: exploiting correlations; throwing away imperceptible information; and encoding the rest as compactly as possible.

Nearby pixels of an image are significantly correlated. After converting the image to a linear colour space, the first step is a rough pass to exploit the linear correlations. A 2×2 tile of pixels can be expressed as a linear combination of the following basis vectors:

```
    +½ +½   +½ -½   +½ +½   +½ -½
    +½ +½   +½ -½   -½ -½   -½ +½
```

This is the Haar basis. The vectors are orthogonal and of norm 1, so the transform does not change the noise ellipsoid; this is an important property. The first of these vectors is a low pass filter. The other three are wavelets, which I call V (Vertical edge), H (Horizontal edge) and C (Chequer pattern). For colour images, each coefficient has three colour channels, but handling colour is outside the scope of this document.

Collecting the low-frequency component of all the 2×2 tiles yields an image of half the resolution and twice the signal-to-noise ratio. The other three components are typically small and much less correlated with each other than the original pixels were. Correlations can be slightly further reduced by mixing coefficients from neighbouring tiles, but that is outside the scope of this document.

The half-resolution image, being an image, still contains significant correlations. Repeating the transform on this image yields a quarter-resolution image, and so on. Eventually (after about five generations) the image is small enough, and of a high enough signal-to-noise ratio, that it is not worth making any effort to compress it. The task is then to compress the five generations of wavelet coefficients, each of which is a grid of (V, H, C) triplets.

The next step is to scale each wavelet coefficient according to how easy it is to perceive a small change. The perceptual model is outside the scope of this document. Here, let's assume that the coefficients are scaled such that the appropriate measure of the difference between two similar images is the sum of the squares of the differences of the wavelet coefficients.

The next step is to quantise the image, i.e. to round the (effectively analogue) input image to the nearest of a discrete set of "representable images". These are the images that can possibly be the output of the decompression algorithm. Since the compressed file is supposed to be small, we want the set of representable images to be as small as possible, while still containing images that are close to any plausible input image. Choosing the set of representable images is the main subject of this document.

The last step is to encode the quantised image into a stream of bits. This involves making a probability model. It is an opportunity to exploit any remaining correlations in the data, including all the non-linear correlations. The probability model is outside the scope of this document.

To summarise, the focus of this document is to choose the set of images onto which the input image is rounded. A reasonable idea (the alternatives are out of scope) is to define a lattice, i.e. a set which is closed under addition and subtraction. The lattice should have as few points as possible, while still offering a reasonable approximation of any plausible image. The quality of the approximation will be measured by the sum of the squares of the errors in the wavelet coefficients: the L2 norm.

## The body-centered cubic lattice

The wavelet coefficients naturally come in (V, H, C) triplets, which may be viewed as points in a 3D space. The best quantisation lattice for a 3D space is the body-centred cubic lattice: hereafter called BCC. Scaled and rotated versions of this lattice are called D3* or A3*. I define BCC to be the set of points with integer coordinates that are either all even or all odd. In other words, BCC is generated by applying the following matrix to Z³:

```
     2  0  0
     0  2  0
     1  1  1
```

The shortest vectors in BCC are (±1, ±1, ±1), of norm 3, and permutations of (±2, 0, 0), of norm 4. The point (1, ½, 0) differs from every point in BCC by a vector of norm at least 1¼, the largest possible. Note that BCC is closed under addition and subtraction, i.e. it is a lattice.

A reasonable idea, which I hope to improve upon, is to define the representable images to be those in which every (V, H, C) triplet is a point of the BCC lattice. This defines a product lattice BCC^341 with 1023 dimensions for every 32×32 tile of pixels (the 1024th dimension is the low frequency component). Its minimal vectors are of norm 3 and 4, just like the BCC lattice. However, if every triplet is pessimal, a point can miss the product lattice by quite a large distance (norm 426¼). The pessimal case is so rare as to be irrelevant, but the average case is also quite bad (TODO: quantify).

The dual lattice BCC* is defined as the set of points whose dot product with every point of BCC is an integer. It is the face-centred cubic lattice generated by the following matrix:

```
     1  0  0
     ½  ½  0
     0  ½  ½
```

Every point of BCC is also in BCC*, so we can form the quotient G = BCC/BCC*, which is called the glue group of the lattice. It contains 16 elements: (0, 0, 0), three that are permutations of (1, 0, 0), and twelve that are permutations of (±½, ±½, 0). Note that G is closed under addition and subtraction modulo BCC.

The significance of G is that by adding one element of G to every point in BCC we obtain a set of points which land in the gaps in BCC. This is used in the construction of the glued lattice.

## A glued lattice

Let's consider a tile of 4×4 pixels. Two generations of the Haar transform resolves if onto the following orthonormal basis vectors:

```
    +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼     +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼
    +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼     +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼
    +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼     -¼ -¼ -¼ -¼     -¼ -¼ +¼ +¼
    +¼ +¼ +¼ +¼     +¼ +¼ -¼ -¼     -¼ -¼ -¼ -¼     -¼ -¼ +¼ +¼

    +½ -½  0  0      0  0 +½ -½      0  0  0  0      0  0  0  0
    +½ -½  0  0      0  0 +½ -½      0  0  0  0      0  0  0  0
     0  0  0  0      0  0  0  0     +½ -½  0  0      0  0 +½ -½
     0  0  0  0      0  0  0  0     +½ -½  0  0      0  0 +½ -½

    +½ +½  0  0      0  0 +½ +½      0  0  0  0      0  0  0  0
    -½ -½  0  0      0  0 -½ -½      0  0  0  0      0  0  0  0
     0  0  0  0      0  0  0  0     +½ +½  0  0      0  0 +½ +½
     0  0  0  0      0  0  0  0     -½ -½  0  0      0  0 -½ -½

    +½ -½  0  0      0  0 +½ -½      0  0  0  0      0  0  0  0
    -½ +½  0  0      0  0 -½ +½      0  0  0  0      0  0  0  0
     0  0  0  0      0  0  0  0     +½ -½  0  0      0  0 +½ -½
     0  0  0  0      0  0  0  0     -½ +½  0  0      0  0 -½ +½
```

The top-left vector is the low-frequency component. The rest of the top row is the (V, H, C) triplet from the second generation, which I will call `p` for "parent". The rest of the four columns are the four (V, H, C) triplets from the first generation, which I will call `a`, `b`, `c` and `d`.

Let me ignore the low-frequency component, and make a quantisation lattice for the other 15 components. I will start with BCC^5. To this, I will add a subgroup of G^5, to fill in the gaps. I will try to add as many vectors as possible without creating any lattice vectors that are shorter than those in BCC^3 (i.e. norm 3).

### Hamming code

The 15-bit Hamming code is a binary linear code (a subgroup of {0, 1}^15) of weight 3 (i.e. every code word has at least three 1s). It can be defined as the set of 15-bit strings that satisfy four parity checks:

```
        0  1  1         0  0  0         1  0  1         0  0  0
     0  0  1  1      0  0  0  0      0  1  0  1      1  1  1  1
     0  0  1  1      1  1  1  1      0  1  0  1      0  0  0  0
     0  0  1  1      1  1  1  1      0  1  0  1      1  1  1  1
```

It therefore has 15 - 4 = 11 generators. If we reorder the wavelet coefficients as follows:

```
       Va Ha Ca
    Vd Vp Cb Hc
    Hd Cc Hp Vb
    Cd Hb Vc Cp
```

We find that five of the generators are in BCC^5:

```
        1  1  1         0  0  0         0  0  0         0  0  0         0  0  0
     0  0  0  0      0  0  1  0      0  0  0  1      1  0  0  0      0  1  0  0
     0  0  0  0      0  0  0  1      0  1  0  0      1  0  0  0      0  0  1  0
     0  0  0  0      0  1  0  0      0  0  1  0      1  0  0  0      0  0  0  1
```
